# Building a Recommendation system: creating an item based collaborative filtering to recommend top 5 movies to users. ğŸš€

![Project Image](https://github.com/AmirFARES/Kaggle-Spaceship-Titanic/blob/main/imgs/spaceship.jpg)

## Introduction ğŸŒŸ
Here is my Phase 4 project for creating a Recommender system. I'm creating a recommender system on an item based collaborative filtering which provides top 5 movie Recommendation to users based on their ratings

## About the Challenge ğŸŒ



### Challenge Details ğŸ“

- **Goal**: Classify passengers as transported or not transported to an alternate dimension.
- **Datasets**: The competition provides a training dataset with personal records and a test dataset for predictions.
- **Evaluation**: Submissions are evaluated based on classification accuracy.

## Project Files ğŸ“‚

Key files related to this project:

- [**train.csv**](./data/train.csv) - Training dataset with personal records and ground truth labels.
- [**test.csv**](./data/test.csv) - Test dataset for predictions.
- [**sample_submission.csv**](./data/sample_submission.csv) - Sample submission file with the required format.
- [**My Notebook**](./spaceship-titanic-weighted-ensemble.ipynb) or [**My Kaggle Notebook**](https://www.kaggle.com/code/amirfares/spaceship-titanic-weighted-ensemble) - My notebook with code and analysis.

## My Approach ğŸš€

1. **Reading Datasets**: I began by loading the provided datasets, both the training and test data.

2. **Handling Missing Values**: I addressed missing data in the dataset, ensuring that no valuable information was lost.

3. **Checking Class Distribution**: To understand the balance between transported and non-transported passengers, I examined the distribution of classes in the training dataset.

4. **Data Visuakization** : I perform some visualization on the train csv to check for the features that affected the rate of transportation on the passengers

<img src="https://github.com/AmirFARES/Kaggle-Spaceship-Titanic/blob/main/imgs/correlationHeatmap.png" alt="Line Chart" width="700" height="437">

5. **Feature Engineering**: To improve model performance, I engineered new features. For example, I extracted additional information from the "numeric" column that had expenditure, summing them to the spending cost 

6. **One-Hot Encoding**: I prepared the categorical data for modeling by performing one-hot encoding, a necessary step for many machine learning models.

7. **Scaling**: I prepared the numeric data for modeling by performing scaling which is a necessary step for data Normalization.

8. **Extracting (X, y)**: I separated the feature matrix (X) and the target variable (y) from the training dataset, ensuring that the data was ready for model training.

9. **Creating the Models**: I created my baseline logistic regression model which was my reference model, then evaluated the accuracy score, plotted the ROC curve. I also improved my baseline model by applying the following model:
     - DecisionTreeClassifier
     - RandomForestClassifier
     - I performed tuning on these models to optimize their performance, checked its accuracy score and finally created the ROC curve 

10. **Making Predictions**: I got the predictions from the three models.

11. **Conclusion and Recommendations**: I gave few conclusion and recommendations i derived from the data


## Connect with Me ğŸ“«

I'm open to collaboration and eager to learn from the data science community. You can find more of my projects on [GitHub](https://github.com/mikabenson/Space_ship-project)).


